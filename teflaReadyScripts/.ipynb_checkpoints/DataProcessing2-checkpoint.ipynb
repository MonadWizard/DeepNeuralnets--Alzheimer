{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Stage-2 [Preparing Tefla ready data]\n",
    "\n",
    "This Notebook runs after the DataProcessing-1.ipynb notebook. This notebook creates the csv file with consists of nii files name and its corresponding label. Then it prepares the data in the tefla ready format, which the adequate distribution of data between training, validation, and test for all the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on:  /home/ubuntu/Norm-Alz/images/Alzheimer/*.nii\n",
      "1\n",
      "Working on:  /home/ubuntu/Norm-Alz/images/Normal/*.nii\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "classes = [\"Alzheimer\", \"Normal\"]\n",
    "#sourceDir = '/home/nagdev/work/siddhant/data/data/'\n",
    "#sourceDir = '/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/'\n",
    "#path_tocsvfile = '/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/'\n",
    "#path_tocsvfile = '/home/nagdev/work/siddhant/data/raw/'\n",
    "#sourceDir = '/home/ec2-user/final_data/images/'\n",
    "#path_tocsvfile = '/home/ec2-user/final_data/'\n",
    "sourceDir = '/home/ubuntu/Norm-Alz/images/'\n",
    "path_tocsvfile = '/home/ubuntu/Norm-Alz/'\n",
    "\n",
    "# 0 represents Alzheimer class, 1 represents MCI class, 2 represents Normal class.\n",
    "count = 0\n",
    "with open(path_tocsvfile + 'all.csv','a') as o:\n",
    "    o.write('nii' + ',' + 'label' +'\\n')\n",
    "    for class_ in classes:\n",
    "        images_path = os.path.join(os.path.join(sourceDir,class_), '*.nii')\n",
    "        print 'Working on: ', images_path\n",
    "        for file_ in glob.glob(images_path):\n",
    "            o.write(file_ + ',' + str(count) +'\\n')\n",
    "        count += 1\n",
    "        print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 nii  label\n",
      "0  /home/ubuntu/Norm-Alz/images/Alzheimer/A86_019...      0\n",
      "1  /home/ubuntu/Norm-Alz/images/Alzheimer/A85_019...      0\n",
      "2  /home/ubuntu/Norm-Alz/images/Alzheimer/A31_006...      0\n",
      "3  /home/ubuntu/Norm-Alz/images/Alzheimer/A28_013...      0\n",
      "4  /home/ubuntu/Norm-Alz/images/Alzheimer/A61_006...      0\n",
      "['/home/ubuntu/Norm-Alz/images/Alzheimer/A14_130_S_4589.nii', '/home/ubuntu/Norm-Alz/images/Alzheimer/A29_010_S_5163.nii', '/home/ubuntu/Norm-Alz/images/Alzheimer/A17_053_S_5070.nii', '/home/ubuntu/Norm-Alz/images/Alzheimer/A79_019_S_4549.nii', '/home/ubuntu/Norm-Alz/images/Alzheimer/A79_019_S_4549.nii', '/home/ubuntu/Norm-Alz/images/Alzheimer/A84_019_S_5019.nii', '/home/ubuntu/Norm-Alz/images/Alzheimer/A100_130_S_4660.nii', '/home/ubuntu/Norm-Alz/images/Alzheimer/A73_130_S_5059.nii', '/home/ubuntu/Norm-Alz/images/Alzheimer/A1_136_S_4993.nii', '/home/ubuntu/Norm-Alz/images/Alzheimer/A43_130_S_4982.nii', '/home/ubuntu/Norm-Alz/images/Alzheimer/A36_002_S_5018.nii']\n",
      "['/home/ubuntu/Norm-Alz/images/Normal/N11_100_S_5246.nii', '/home/ubuntu/Norm-Alz/images/Normal/N94_006_S_4357.nii', '/home/ubuntu/Norm-Alz/images/Normal/N147_018_S_4349.nii', '/home/ubuntu/Norm-Alz/images/Normal/N149_18_S_4399.nii', '/home/ubuntu/Norm-Alz/images/Normal/N97_053_S_4578.nii', '/home/ubuntu/Norm-Alz/images/Normal/N116_010_S_4442.nii', '/home/ubuntu/Norm-Alz/images/Normal/N92_006_S_4357.nii', '/home/ubuntu/Norm-Alz/images/Normal/N159_019_S_4367.nii', '/home/ubuntu/Norm-Alz/images/Normal/N99_006_S_4449.nii', '/home/ubuntu/Norm-Alz/images/Normal/N112_010_S_4345.nii', '/home/ubuntu/Norm-Alz/images/Normal/N2_136_S_4433.nii', '/home/ubuntu/Norm-Alz/images/Normal/N67_002_S_4213.nii', '/home/ubuntu/Norm-Alz/images/Normal/N159_019_S_4367.nii', '/home/ubuntu/Norm-Alz/images/Normal/N112_010_S_4345.nii']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "# script to create a tefla compatible data dir for training and validation data\n",
    "#sourceDir = '/home/nagdev/work/siddhant/data/raw/'\n",
    "#destDir = '/home/nagdev/work/siddhant/data/raw/processed/'\n",
    "#sourceDir = '/home/ec2-user/final_data/'\n",
    "#destDir = '/home/ec2-user/final_data/processed/'\n",
    "sourceDir = '/home/ubuntu/Norm-Alz/'\n",
    "destDir = '/home/ubuntu/Norm-Alz/processed/'\n",
    "\n",
    "categories_picked_for_validation_data = [0,1]\n",
    "#categories_picked_for_validation_data = [0,1,2] # if we use all three categories alzheimer, mci and normal\n",
    "validation_data_percentage = 10\n",
    "\n",
    "def create_tefla_data(source_dir,destination_dir,validation_categories,validation_percentage):\n",
    "    #if os.path.exists(destination_dir):\n",
    "    #    shutil.rmtree(destination_dir)\n",
    "\n",
    "    training_dir = destination_dir + 'training_64/'\n",
    "    validation_dir = destination_dir + 'validation_64/'\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(training_dir)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.makedirs(validation_dir)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    labels = pd.read_csv( source_dir + 'all.csv')\n",
    "    print labels.head()\n",
    "    validation_dict = {}\n",
    "\n",
    "    for c in validation_categories:\n",
    "        validation_dict[c] = []\n",
    "    # now test_set has some nii files inside it from each class\n",
    "\n",
    "    #validation set creation\n",
    "    validation_set = []\n",
    "\n",
    "    for i, rows in labels.iterrows():\n",
    "        if validation_dict.has_key(rows['label']):\n",
    "            validation_dict[rows['label']].append(rows['nii'])\n",
    "\n",
    "    for l in validation_dict:\n",
    "        np.random.seed(0)\n",
    "        n = math.ceil(validation_percentage * len(validation_dict[l]) / 100.0)\n",
    "        random_array = np.random.choice(validation_dict[l],int(n))\n",
    "        validation_set = validation_set + random_array.tolist()\n",
    "        print random_array.tolist()\n",
    "    # add some of the nii files to validation set which are not part of test set.\n",
    "\n",
    "    training_images = []\n",
    "    training_labels = []\n",
    "    validation_images = []\n",
    "    validation_labels =[]\n",
    "    \n",
    "    for i, rows in labels.iterrows():\n",
    "        if rows['nii'] in validation_set:\n",
    "            #print rows['nii']\n",
    "            for file_ in glob.glob(os.path.join(rows['nii'], '*.jpg')):\n",
    "                validation_images.append(os.path.basename(file_).replace('.jpg', ''))\n",
    "                validation_labels.append(rows['label'])\n",
    "                #print file_\n",
    "                process_and_save_image(file_, validation_dir + os.path.basename(file_).replace('.jpg','') + \".jpg\")\n",
    "        else:\n",
    "            #print rows['nii']\n",
    "            for file_ in glob.glob(os.path.join(rows['nii'], '*.jpg')):\n",
    "                training_images.append(os.path.basename(file_).replace('.jpg', ''))\n",
    "                training_labels.append(rows['label'])\n",
    "                process_and_save_image(file_, training_dir + os.path.basename(file_).replace('.jpg','') + \".jpg\")\n",
    "                \n",
    "    header = ['image', 'label']\n",
    "\n",
    "    # saving training csv\n",
    "    training_out = np.column_stack((training_images, training_labels))\n",
    "    training_out = np.row_stack((header, training_out))\n",
    "    np.savetxt(destination_dir + 'training_labels.csv', training_out, delimiter=',', fmt='%s')\n",
    "\n",
    "    # saving validation csv\n",
    "    validation_out = np.column_stack((validation_images, validation_labels))\n",
    "    validation_out = np.row_stack((header, validation_out))\n",
    "    np.savetxt(destination_dir + 'validation_labels.csv', validation_out, delimiter=',', fmt='%s')\n",
    "\n",
    "def process_and_save_image(source_path,destination_path):\n",
    "    img = resize(source_path, 64)\n",
    "    img.save(destination_path, quality=100)\n",
    "\n",
    "def resize(fname, target_size):\n",
    "    # print('Processing image: %s' % fname)\n",
    "    img = Image.open(fname)\n",
    "    return img\n",
    "\n",
    "#calling method\n",
    "create_tefla_data(sourceDir,destDir,categories_picked_for_validation_data,validation_data_percentage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
