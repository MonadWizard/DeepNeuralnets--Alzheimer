{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Stage-2 [Preparing Tefla ready data]\n",
    "\n",
    "This Notebook runs after the DataProcessing-1.ipynb notebook. This notebook creates the csv file with consists of image name and its corresponding label. Then it prepares the data in the tefla ready format, which the adequate distribution of data between training, validation, and test for all the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on:  /home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/Alzheimer/*.nii\n",
      "1\n",
      "Working on:  /home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/MCI/*.nii\n",
      "2\n",
      "Working on:  /home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/Normal/*.nii\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "classes = [\"Alzheimer\", \"MCI\", \"Normal\"]\n",
    "#sourceDir = '/home/nagdev/work/siddhant/data/raw/'\n",
    "sourceDir = '/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/'\n",
    "path_tocsvfile = '/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/'\n",
    "\n",
    "# 0 represents Alzheimer class, 1 represents MCI class, 2 represents Normal class.\n",
    "count = 0\n",
    "with open(path_tocsvfile + 'all.csv','a') as o:\n",
    "    o.write('nii' + ',' + 'label' +'\\n')\n",
    "    for class_ in classes:\n",
    "        images_path = os.path.join(os.path.join(sourceDir,class_), '*.nii')\n",
    "        print 'Working on: ', images_path\n",
    "        for file_ in glob.glob(images_path):\n",
    "            o.write(file_ + ',' + str(count) +'\\n')\n",
    "        count += 1\n",
    "        print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 nii  label\n",
      "0  /home/h_hack/work/pywork/ML/datasets/Alzheimer...      0\n",
      "1  /home/h_hack/work/pywork/ML/datasets/Alzheimer...      0\n",
      "2  /home/h_hack/work/pywork/ML/datasets/Alzheimer...      0\n",
      "3  /home/h_hack/work/pywork/ML/datasets/Alzheimer...      0\n",
      "4  /home/h_hack/work/pywork/ML/datasets/Alzheimer...      0\n",
      "{0: ['/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/Alzheimer/A26_018_S_4733_result.nii', '/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/Alzheimer/A6_130_S_4997_result.nii', '/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/Alzheimer/A28_013_S_5071_result.nii', '/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/Alzheimer/A2_131_S_5138_result.nii', '/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/Alzheimer/A8_130_S_4984_result.nii'], 1: ['/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/MCI/M7_006_S_4515_result.nii', '/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/MCI/M8_006_S_4363_result.nii', '/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/MCI/M10_002_S_4799_result.nii', '/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/MCI/M12_002_S_4654_result.nii', '/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/MCI/M4_006_S_4960_result.nii'], 2: ['/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/Normal/N15_002_S_4262_result.nii', '/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/Normal/N22_031_S_4032_result.nii', '/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/Normal/N1_136_S_4727_result.nii', '/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/Normal/N5_130_S_4343_result.nii']}\n",
      "['/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/Alzheimer/A8_130_S_4984_result.nii', '/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/MCI/M4_006_S_4960_result.nii', '/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/Normal/N15_002_S_4262_result.nii']\n",
      "['/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/Alzheimer/A26_018_S_4733_result.nii', '/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/MCI/M7_006_S_4515_result.nii', '/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/Normal/N22_031_S_4032_result.nii']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "# script to create a tefla compatible data dir for training and validation data\n",
    "sourceDir = '/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/'\n",
    "destDir = '/home/h_hack/work/pywork/ML/datasets/Alzheimer_detect/fsl_preprocessed/images/'\n",
    "\n",
    "categories_picked_for_validation_data = [0,1,2]\n",
    "categories_picked_for_test_data = [0,1,2]\n",
    "validation_data_percentage = 10\n",
    "test_data_percentage = 20\n",
    "\n",
    "def create_tefla_data(source_dir,destination_dir,validation_categories,test_categories,validation_percentage,test_percentage):\n",
    "    #if os.path.exists(destination_dir):\n",
    "    #    shutil.rmtree(destination_dir)\n",
    "\n",
    "    training_dir = destination_dir + 'training_224/'\n",
    "    validation_dir = destination_dir + 'validation_224/'\n",
    "    test_dir = destination_dir + 'test_224/'\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(training_dir)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.makedirs(validation_dir)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.makedirs(test_dir)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    labels = pd.read_csv( source_dir + 'all.csv')\n",
    "    print labels.head()\n",
    "    validation_dict = {}\n",
    "    test_dict = {}\n",
    "\n",
    "    for c in validation_categories:\n",
    "        validation_dict[c] = []\n",
    "\n",
    "    for c in test_categories:\n",
    "        test_dict[c] = []\n",
    "\n",
    "\n",
    "    # test set creation\n",
    "    test_set = []\n",
    "\n",
    "    for i, rows in labels.iterrows():\n",
    "        if test_dict.has_key(rows['label']):\n",
    "            test_dict[rows['label']].append(rows['nii'])\n",
    "    print test_dict\n",
    "    for k in test_dict:\n",
    "        np.random.seed(0)\n",
    "        n = math.ceil(test_percentage * len(test_dict[k]) / 100.0)\n",
    "        random_array = np.random.choice(test_dict[k],int(n))\n",
    "        test_set = test_set + random_array.tolist()\n",
    "    \n",
    "    print test_set\n",
    "    # now test_set has some nii files inside it from each class\n",
    "\n",
    "    #validation set creation\n",
    "    validation_set = []\n",
    "\n",
    "    for i, rows in labels.iterrows():\n",
    "        if rows['nii'] not in test_set:\n",
    "            if validation_dict.has_key(rows['label']):\n",
    "                validation_dict[rows['label']].append(rows['nii'])\n",
    "\n",
    "    for l in validation_dict:\n",
    "        np.random.seed(0)\n",
    "        n = math.ceil(validation_percentage * len(validation_dict[l]) / 100.0)\n",
    "        random_array = np.random.choice(validation_dict[l],int(n))\n",
    "        validation_set = validation_set + random_array.tolist()\n",
    "    print validation_set\n",
    "    # add some of the nii files to validation set which are not part of test set.\n",
    "\n",
    "    training_images = []\n",
    "    training_labels = []\n",
    "    validation_images = []\n",
    "    validation_labels =[]\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "\n",
    "    for i, rows in labels.iterrows():\n",
    "        if rows['nii'] in test_set:\n",
    "            #print rows['nii']\n",
    "            for file_ in glob.glob(os.path.join(rows['nii'], '*.jpg')):\n",
    "                test_images.append(os.path.basename(file_).replace('.jpg', ''))\n",
    "                test_labels.append(rows['label'])\n",
    "                #print file_\n",
    "                process_and_save_image(file_, test_dir + os.path.basename(file_).replace('.jpg','') + \".tiff\")\n",
    "        elif rows['nii'] in validation_set:\n",
    "            #print rows['nii']\n",
    "            for file_ in glob.glob(os.path.join(rows['nii'], '*.jpg')):\n",
    "                validation_images.append(os.path.basename(file_).replace('.jpg', ''))\n",
    "                validation_labels.append(rows['label'])\n",
    "                #print file_\n",
    "                process_and_save_image(file_, validation_dir + os.path.basename(file_).replace('.jpg','') + \".tiff\")\n",
    "        else:\n",
    "            #print rows['nii']\n",
    "            for file_ in glob.glob(os.path.join(rows['nii'], '*.jpg')):\n",
    "                training_images.append(os.path.basename(file_).replace('.jpg', ''))\n",
    "                training_labels.append(rows['label'])\n",
    "                process_and_save_image(file_, training_dir + os.path.basename(file_).replace('.jpg','') + \".tiff\")\n",
    "                \n",
    "    header = ['image', 'label']\n",
    "\n",
    "    # saving training csv\n",
    "    training_out = np.column_stack((training_images, training_labels))\n",
    "    training_out = np.row_stack((header, training_out))\n",
    "    np.savetxt(destination_dir + 'training_labels.csv', training_out, delimiter=',', fmt='%s')\n",
    "\n",
    "    # saving validation csv\n",
    "    validation_out = np.column_stack((validation_images, validation_labels))\n",
    "    validation_out = np.row_stack((header, validation_out))\n",
    "    np.savetxt(destination_dir + 'validation_labels.csv', validation_out, delimiter=',', fmt='%s')\n",
    "\n",
    "    # saving testing csv\n",
    "    test_out = np.column_stack((test_images, test_labels))\n",
    "    test_out = np.row_stack((header, test_out))\n",
    "    np.savetxt(destination_dir + 'test_labels.csv', test_out, delimiter=',', fmt='%s')\n",
    "\n",
    "def process_and_save_image(source_path,destination_path):\n",
    "    img = resize(source_path, 224)\n",
    "    img.save(destination_path, quality=100)\n",
    "\n",
    "def resize(fname, target_size):\n",
    "    # print('Processing image: %s' % fname)\n",
    "    img = Image.open(fname)\n",
    "    return img\n",
    "\n",
    "#calling method\n",
    "create_tefla_data(sourceDir,destDir,categories_picked_for_validation_data,categories_picked_for_test_data,validation_data_percentage,test_data_percentage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
